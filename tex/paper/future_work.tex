\section{Future Work}
The results presented here lay a reasonable groundwork for future work. The most straightforward thing to try next would be to test my model's performance on other datasets. In fact, I originally had planned to test on the CIFAR-10 color image dataset \cite{cifar}, but I discovered that the state-of-art method used 4000 clusters, which was outside of my computational budget. Additionally, it would be interesting to test my model on non-image datasets, such as textual data.

Another area to pursue would be to experiment with the autoencoder architecture. The results presented here use a very basic autoencoder, but significantly more advanced models exist. The most straightforward change would be to implement  a deep autoencoder which has many more hidden layers. Additionally, there are has been some work in using deconvolution operations which could be especially useful for the image datasets used here. In any case, it seems likely that a more sophisticated architecture could produce better lower dimensional representations, leading to better cluster performance.

\section{Conclusion}
In closing, $k$-means can perform poorly on datasets with many features or in the presence of noise. I designed and implemented autoencoded $k$-means, a neural architecture where jointly learns an autoencoder and optimizes the $k$-means objective. We see that on two image datasets, this new model performs better than $k$-means by clustering the data in a lower dimensional space. Furthermore, autoencoded $k$-means can successfully learn the noise distribution of noisy data leading to a denoising effect. This results in significantly improved performance in the presence of noise. 